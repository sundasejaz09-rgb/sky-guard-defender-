# ======================
# 1. Install dependencies
# ======================
!pip install ultralytics roboflow --quiet

# ======================
# 2. Import Libraries
# ======================
import os
import shutil
from roboflow import Roboflow
from ultralytics import YOLO
import yaml
from sklearn.model_selection import train_test_split
from google.colab import drive # ðŸ‘ˆ ADDED: Import for Google Drive mounting

# ======================
# ðŸš€ ADDED: Mount Google Drive
# ======================
print("ðŸ”— Mounting Google Drive...")
# Mount Google Drive at the default location
drive.mount('/content/drive')
print("âœ… Google Drive Mounted!")

# Define a persistent path for saving results in your Drive
# You can change 'YOLO_PROJECTS' to any folder name in your 'My Drive'
GDRIVE_PATH = "/content/drive/My Drive/YOLO_PROJECTS/NASA_Flood_Detection"
os.makedirs(GDRIVE_PATH, exist_ok=True)

# Change the current working directory to the Drive path
# This ensures that the trained weights are saved directly to your Google Drive.
%cd "{GDRIVE_PATH}"
print(f"Current working directory changed to: {os.getcwd()}")
print("-----------------------------------------------------")

# ======================
# 3. Set Roboflow API Key
# ======================
# Note: Roboflow key is set globally in the OS environment.
os.environ["ROBOFLOW_API_KEY"] = "x7xjPs1PnEKkL8X7P4uJ"  # ðŸ”‘ your key
print("âœ… API Key Set!")

# ======================
# 4. Download Dataset via Roboflow SDK
# ======================
# The dataset will download to the current working directory, which is now in Google Drive.
rf = Roboflow(api_key=os.environ["ROBOFLOW_API_KEY"])
project = rf.workspace("superior-university-xqg9u").project("flood-detect-gtvdk-barpt")
# The dataset will download to a subdirectory within the mounted drive path
dataset = project.version(1).download("yolov8")

print("âœ… Dataset downloaded at:", dataset.location)

# ======================
# 5. Custom Train-Val-Test Split (70-20-10)
# ======================
def create_custom_split(dataset_path, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):
    """
    Create custom train-val-test split (70-20-10)
    """
    # Read original data.yaml
    with open(f"{dataset_path}/data.yaml", 'r') as f:
        data = yaml.safe_load(f)

    # Get
    all_images_root = os.path.join(dataset_path, 'train', 'images')
    if not os.path.exists(all_images_root):
        # Handle case where all images might be in 'images' folder at dataset root
        all_images_root = os.path.join(dataset_path, 'images')

    if not os.path.exists(all_images_root):
        print(f"Error: Could not find image directory at {os.path.join(dataset_path, 'train', 'images')} or {os.path.join(dataset_path, 'images')}")
        # Assuming the original Roboflow structure is flattened into one 'train' dir for all images
        images_dir = os.path.join(dataset_path, 'images')
        labels_dir = os.path.join(dataset_path, 'labels')
    else:
        images_dir = all_images_root
        labels_dir = images_dir.replace('images', 'labels')

    if not os.path.exists(images_dir):
        print("Error: Image directory not found. Using Roboflow's structure directly.")
        # Revert to using the downloaded 'train' folder as the source if custom split is required
        images_dir = os.path.join(dataset_path, data['train'].split('/')[0], 'images')
        labels_dir = os.path.join(dataset_path, data['train'].split('/')[0], 'labels')

    image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]

    # Split the data
    train_files, temp_files = train_test_split(image_files, train_size=train_ratio, random_state=42)
    val_files, test_files = train_test_split(temp_files, train_size=val_ratio/(val_ratio+test_ratio), random_state=42)

    # Create new directories
    splits = ['train', 'val', 'test']
    # Define new root for split data
    new_data_root = os.path.join(dataset_path, 'custom_split')
    os.makedirs(new_data_root, exist_ok=True)

    for split in splits:
        os.makedirs(f"{new_data_root}/images/{split}", exist_ok=True)
        os.makedirs(f"{new_data_root}/labels/{split}", exist_ok=True)

    # Copy files to respective directories
    for split, files in zip(splits, [train_files, val_files, test_files]):
        for img_file in files:
            # Determine source paths
            label_file = img_file.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt')
            src_img_path = os.path.join(images_dir, img_file)
            src_label_path = os.path.join(labels_dir, label_file)

            # Determine destination paths
            dst_img_path = os.path.join(new_data_root, 'images', split, img_file)
            dst_label_path = os.path.join(new_data_root, 'labels', split, label_file)

            # Copy image
            shutil.copy2(src_img_path, dst_img_path)

            # Copy corresponding label file
            if os.path.exists(src_label_path):
                shutil.copy2(src_label_path, dst_label_path)

    # Update data.yaml to point to the new custom split directories
    data['train'] = os.path.join(new_data_root, 'images', 'train')
    data['val'] = os.path.join(new_data_root, 'images', 'val')
    data['test'] = os.path.join(new_data_root, 'images', 'test')

    with open(f"{dataset_path}/data.yaml", 'w') as f:
        yaml.dump(data, f)

    print(f"âœ… Custom split created: Train({len(train_files)}), Val({len(val_files)}), Test({len(test_files)})")
    return data

# Create custom split
updated_data = create_custom_split(dataset.location)

# ======================
# 6. Train YOLOv11 with EPA Techniques
# ======================
model = YOLO("yolo11n.pt")

results = model.train(
    data=f"{dataset.location}/data.yaml",
    epochs=30,
    imgsz=640,
    batch=16,

    # ðŸ”§ EPA (Efficient Projection and Augmentation) Techniques
    # 1. Advanced Augmentation
    augment=True,
    hsv_h=0.015,       # HSV-Hue augmentation
    hsv_s=0.7,         # HSV-Saturation augmentation
    hsv_v=0.4,         # HSV-Value augmentation
    degrees=10.0,      # Rotation degrees
    translate=0.1,     # Image translation
    scale=0.5,         # Image scale
    shear=2.0,         # Image shear
    perspective=0.001, # Perspective transformation
    flipud=0.0,        # Flip up-down
    fliplr=0.5,        # Flip left-right
    mosaic=1.0,        # Mosaic augmentation
    mixup=0.0,         # MixUp augmentation

    # 2. Efficient Training Parameters
    cos_lr=True,       # Cosine learning rate scheduler
    close_mosaic=10,   # Disable mosaic last 10 epochs
    overlap_mask=True,
    lr0=0.01,          # Initial learning rate
    lrf=0.01,          # Final learning rate
    patience=50,       # Early stopping patience

    # 3. Regularization
    dropout=0.1,       # Dropout for regularization
    weight_decay=0.0005,

    # 4. Project Configuration
    # The project will be created in the current working directory, which is in Google Drive.
    project=".",
    name="YOLOv11_Flood_EPA",

    # 5. Additional EPA optimizations
    single_cls=False,
    optimizer="auto",  # Auto-select best optimizer
    verbose=True,
    save_period=10,    # Save checkpoint every 10 epochs
)

print("âœ… Training complete with EPA techniques!")
# Update the path to reflect the Google Drive location
print(f"ðŸ“‚ Best model saved at: {GDRIVE_PATH}/YOLOv11_Flood_EPA/weights/best.pt")

# ======================
# 7. Evaluate on Test Set
# ======================
print("ðŸ§ª Evaluating on test set...")
test_results = model.val(
    data=f"{dataset.location}/data.yaml",
    split='test'
)

print(f"ðŸ“Š Test Results - mAP50: {test_results.box.map50:.4f}, mAP50-95: {test_results.box.map:.4f}")
